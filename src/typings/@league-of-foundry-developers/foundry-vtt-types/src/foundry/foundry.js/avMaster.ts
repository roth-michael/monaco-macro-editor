
    export default [
      "@league-of-foundry-developers/foundry-vtt-types/src/foundry/foundry.js/avMaster.d.ts",
      "/**\n * The master Audio/Video controller instance.\n * This is available as the singleton game.webrtc\n */\ndeclare class AVMaster {\n  constructor();\n\n  settings: AVSettings;\n\n  config: AVConfig;\n\n  /**\n   * The Audio/Video client class\n   */\n  client: InstanceType<CONFIG['WebRTC']['clientClass']>;\n\n  /**\n   * A flag to track whether the current user is actively broadcasting their microphone.\n   * @defaultValue `false`\n   */\n  broadcasting: boolean;\n\n  /**\n   * Flag to determine if we are connected to the signalling server or not.\n   * This is required for synchronization between connection and reconnection attempts.\n   * @defaultValue `false`\n   * @internal\n   */\n  protected _connected: boolean;\n\n  /**\n   * A flag to track whether the A/V system is currently in the process of reconnecting.\n   * This occurs if the connection is lost or interrupted.\n   * @defaultValue `false`\n   * @internal\n   */\n  protected _reconnecting: boolean;\n\n  /**\n   * @defaultValue `{}`\n   * @internal\n   */\n  protected _speakingData: AVMaster.SpeakingData;\n\n  /**\n   * @defaultValue `{}`\n   * @internal\n   */\n  protected _pttHandlers: AVMaster.PTTHandlers;\n\n  /**\n   * @defaultValue `0`\n   * @internal\n   */\n  protected _pttMuteTimeout: number;\n\n  get mode(): AVSettings.VoiceMode;\n\n  /**\n   * Connect to the Audio/Video client.\n   * @returns Was the connection attempt successful?\n   */\n  connect(): Promise<boolean>;\n\n  /**\n   * Disconnect from the Audio/Video client.\n   * @returns Whether an existing connection was terminated?\n   */\n  disconnect(): Promise<boolean>;\n\n  /**\n   * Callback actions to take when the user becomes disconnected from the server.\n   */\n  reestablish(): Promise<void>;\n\n  /**\n   * Initialize the local broadcast state.\n   * @internal\n   */\n  protected _initialize(): void;\n\n  /**\n   * A user can broadcast audio if the AV mode is compatible and if they are allowed to broadcast.\n   */\n  canUserBroadcastAudio(userId: string): boolean;\n\n  /**\n   * A user can share audio if they are allowed to broadcast and if they have not muted themselves or been blocked.\n   */\n  canUserShareAudio(userId: string): boolean;\n\n  /**\n   * A user can broadcast video if the AV mode is compatible and if they are allowed to broadcast.\n   */\n  canUserBroadcastVideo(userId: string): boolean;\n\n  /**\n   * A user can share video if they are allowed to broadcast and if they have not hidden themselves or been blocked.\n   */\n  canUserShareVideo(userId: string): boolean;\n\n  /**\n   * Trigger a change in the audio broadcasting state when using a push-to-talk workflow.\n   * @param intent - The user's intent to broadcast. Whether an actual broadcast occurs will depend\n   *                 on whether or not the user has muted their audio feed.\n   */\n  broadcast(intent: boolean): void;\n\n  /**\n   * Set up audio level listeners to handle voice activation detection workflow.\n   * @param mode - The currently selected voice broadcasting mode\n   * @internal\n   */\n  protected _initializeUserVoiceDetection(mode: AVSettings.VoiceMode): void;\n\n  /**\n   * Activate voice detection tracking for a userId on a provided MediaStream.\n   * Currently only a MediaStream is supported because MediaStreamTrack processing is not yet supported cross-browser.\n   * @param userId - The Foundry User ID whose voice is being processed\n   * @param stream - The MediaStream which corresponds to that User\n   * @param ms     - A number of milliseconds which represents the voice activation volume interval\n   *                 (default: `CONFIG.WebRTC.detectPeerVolumeInterval`)\n   */\n  activateVoiceDetection(userId: string, stream: MediaStream, ms?: number): void;\n\n  /**\n   * Actions which the orchestration layer should take when a peer user disconnects from the audio/video service.\n   * @param userId - The id of the disconnecting User\n   */\n  deactivateVoiceDetection(userId: string): void;\n\n  /**\n   * Periodic notification of user audio level\n   *\n   * This function uses the audio level (in dB) of each stream it's listening to to determine if a user\n   * is speaking or not and notifies the UI of such changes.\n   *\n   * The User is considered speaking if they are above the decibel threshold in any of the history values.\n   * This marks them as speaking as soon as they have a high enough volume, and marks them as not speaking only after\n   * they drop below the threshold in all histories (last 4 volumes = for 200 ms).\n   *\n   * There can be more optimal ways to do this and which uses whether the user was already considered speaking before\n   * or not, in order to eliminate short bursts of audio (coughing for example).\n   *\n   * @param userId  - The user ID of the user whose audio levels are being reported\n   * @param dbLevel - The audio level in decibels of the user within the last 50ms\n   * @internal\n   */\n  protected _onAudioLevel(userId: string, dbLevel: number): void;\n\n  /**\n   * Set up interactivity and handling of push-to-talk broadcasting workflow.\n   * @internal\n   */\n  protected _initializePushToTalk(): void;\n\n  /**\n   * Resets the speaking history of a user\n   * If the user was considered speaking, then mark them as not speaking\n   * @param userId - The ID of the user\n   * @internal\n   */\n  protected _resetSpeakingHistory(userId: string): void;\n\n  /**\n   * Handle activation of a push-to-talk key or button.\n   * @param event - The original keydown event\n   * @internal\n   */\n  _onPTTStart(event: KeyboardEvent | MouseEvent): void;\n\n  /**\n   * Handle deactivation of a push-to-talk key or button.\n   * @param event - The original keyup event\n   * @internal\n   */\n  _onPTTEnd(event: KeyboardEvent | MouseEvent): void;\n\n  /**\n   * Handle matching old and new PTT configurations against the mouse or keyboard event.\n   * @param event - The original event\n   * @internal\n   */\n  _isPTTKey(event: KeyboardEvent | MouseEvent): boolean;\n\n  render(): void;\n\n  /**\n   * Render the audio/video streams to the CameraViews UI.\n   * Assign each connected user to the correct video frame element.\n   */\n  onRender(): void;\n\n  /**\n   * Respond to changes which occur to AV Settings.\n   * Changes are handled in descending order of impact.\n   * @param changed - The object of changed AV settings\n   */\n  onSettingsChanged(changed: DeepPartial<AVSettings.Settings>): void;\n\n  debug(message: string): void;\n}\n\ndeclare namespace AVMaster {\n  type SpeakingData = Partial<Record<string, { speaking: boolean; volumeHistories: number[] }>>;\n  type PTTHandler = (event: KeyboardEvent | MouseEvent) => void;\n  type PTTHandlers =\n    | {}\n    | { mousedown: PTTHandler; mouseup: PTTHandler }\n    | {\n        keydown: PTTHandler;\n        keyup: PTTHandler;\n      };\n}\n"
    ]
  